# DA-in-GameDev-lab5
АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5

Интеграция экономической системы в проект Unity и обучение ML-Agent.

Выполнил:
- Зубов Алексей Иванович
- РИ-210946 
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 80 |
| Задание 2 | * | 20 |


знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨


Цель работы
Научиться интегрировать экономическую систему в проект Unity и обучать ML-Agent.

## Задание 1
## Измените параметры файла. yaml-агента и определить какие параметры и как влияют на обучение модели.
 
Открыл проект и подключил библиотеки мл агента. image

Добавил Economic.yaml в папку с проектом. Содержимое файла Economic.yaml:

    behaviors:
      Economic:
        trainer_type: ppo
        hyperparameters:
          batch_size: 1024
          buffer_size: 10240
          learning_rate: 3.0e-4
          learning_rate_schedule: linear
          beta: 1.0e-2
          epsilon: 0.2
          lambd: 0.95
          num_epoch: 3      
        network_settings:
          normalize: false
          hidden_units: 128
          num_layers: 2
        reward_signals:
          extrinsic:
            gamma: 0.99
            strength: 1.0
        checkpoint_interval: 500000
        max_steps: 750000
        time_horizon: 64
        summary_freq: 5000
        self_play:
          save_steps: 20000
          team_change: 100000
          swap_steps: 10000
          play_against_latest_model_ratio: 0.5
          window: 10


## Задание 2
## Опишите результаты, выведенные в TensorBoard.
Environment
Cumulative Reward - среднее общее вознаграждение за эпизод для всех агентов. Увеличивается, когда эпизод обучения успешен. График должен постоянно увеличиваться, но может вести себя скачкообразно.

Episode Length - средняя продолжительность эпизода обучения в среде для агентов.

Losses
Policy Loss - средняя величина функции потери политики, где политика - процесс принятия решений. График должен идти вниз во время успешного эпизода.

Value Loss - средняя потеря функции значения. Она моделирует, насколько хорошо агент прогнозирует значение своего следующего состояния. Должна увеличиваться, пока агент обучается, а затем уменьшаться, когда вознаграждение стабилизируется.

Policy
Entropy - график случайности решений модели. Должен уменьшаться во время успешного эпизода.
Beta - гиперпараметр для настройки Entropy.
Epsilon - гиперпараметр, влияет на скорость развития политики.
Extrinsic Reward - соответствует среднему совокупному вознаграждению, полученному от окружающей среды за эпизод.
Value Estimate - это среднее значение, посещённое всеми состояниями агента. Чтобы отражать увеличение знаний агента, это значение должно расти, а затем стабилизироваться.
Learning Rate - показывает величину шага при поиске оптимальной политики. Должен уменьшаться линейно.
Self play
ELO - показывает силу сети.
Выводы
В этой лабораторной я научился интегрировать экономическую систему в проект Unity, используя мл агента. Научился выводить графики в TensorBoard и анализировать их.

**BigDigital Team: Denisov | Fadeev | Panov**
